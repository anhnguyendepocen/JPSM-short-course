---
title: "Introduction to Big Data for Social Science"
subtitle: "Web scraping"
author: "Christoph Kern"
output: html_notebook
---

## Setup

We start by first installing some packages that we will need throughout this notebook.

```{r}
# install.packages("xml2")
# install.packages("rvest")
# install.packages("jsonlite")
# install.packages("robotstxt")
```

Besides installing the packages, they also have to be loaded.

```{r}
library(xml2)
library(rvest)
library(jsonlite)
library(robotstxt)
```

## Web Scraping Example

### Single website

In this example, we scrape some information from niche.com on colleges in the US. This website reports college ratings and presents the results in a relatively structured list, which eases our scraping task. We first check whether robots are allowed on their webpage.

```{r}
paths_allowed("https://www.niche.com/colleges/search/best-colleges/")
```

We then start by reading in the information from the first results page (search without filters).

```{r}
url <- read_html("https://www.niche.com/colleges/search/best-colleges/")
```

Next, lets try to extract the college names from this page. To create the corresponding xpath, https://selectorgadget.com/ can be used by clicking on the desired element of the webpage. We then copy the resulting xpath into the call to `html_nodes()`.

```{r}
nds <- html_nodes(url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "search-result__title", " " ))]')
```

This object is still a list, so we have to extract further to convert it to the desired format.

```{r}
names <- html_text(nds)
head(names)
```

Done (already)! However, obviously there is more on this website that we want to extract. In the next chunk, we look at the acceptance rate, net price and SAT range for the colleges. We proceed as earlier by using https://selectorgadget.com/ to get the corresponding xpath. 

```{r}
nds2 <- html_nodes(url, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "search-result-fact-list__item", " " ))]')
```

Again, we extract the text from the results object.

```{r}
stats <- html_text(nds2)
head(stats)
```

Seems like we are moving in the right direction, but we might want to re-structure this character vector and built a (cleaner) `data.frame`. For this, we first convert to a 4-column `matrix`.  

```{r}
stats_dat <- matrix(stats, ncol = 4, byrow = T)
head(stats_dat)
```

Now we create a `data.frame` and assign variable names.

```{r}
stats_dat <- as.data.frame(stats_dat)
names(stats_dat) <- c("niche_grade", "acceptance_rate", "net_price", "SAT_range")
```

We probably want to clean-up the variables and e.g. get rid of non-numeric content. This can be done by using regular expressions via `gsub()`.

```{r}
stats_dat$niche_grade <- gsub("Overall Niche Grade", "", stats_dat$niche_grade)
stats_dat$acceptance_rate <- as.numeric(gsub("[^0123456789,]", "", stats_dat$acceptance_rate))
stats_dat$net_price <- as.numeric(gsub("[^0123456789]", "", stats_dat$net_price))
stats_dat$SAT_range <- gsub("SAT Range", "", stats_dat$SAT_range)
head(stats_dat)
```

Although we could do more tidying here, lets just merge this data with the college names we scraped earlier and create a (somewhat) final data set.

```{r}
top_colleges <- data.frame(names, stats_dat)
str(top_colleges)
```

### Looping over multiple pages

When searching for colleges, the full list of results is spread over multiple pages. To scrape the same type of information from multiple pages we can embed the previous steps in a loop. We first specify the number of pages to loop over (e.g., 10) and define an empty `data.frame` to store the results.

```{r}
npages <- 10
colleges <- data.frame()
```

Now we run all previous steps within a for-loop. The key component here is that since we need a different url for each page, we use a counter and modify the url by pasting in a new page number (from one to ten) in every iteration of the loop. The other components of the loop are essentially the same as before, minus most of the data cleaning to keep things readable.

```{r}
for(i in 1:npages) {
  url <- paste0("https://www.niche.com/colleges/search/best-colleges/?page=",i, sep = "")
  src <- read_html(url)
  print(url)
  
  nds <- html_nodes(src, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "search-result__title", " " ))]')
  names <- html_text(nds)
  
  nds2 <- html_nodes(src, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "search-result-fact-list__item", " " ))]')
  stats <- html_text(nds2)
  stats_dat <- as.data.frame(matrix(stats, ncol = 4, byrow = T))

  part <- data.frame(names, stats_dat)
  colleges <- rbind(colleges, part)
}
```

Note that the resulting `data.frame` now has 270 rows, since we scraped over 10 pages (each with 27 results) instead of using just one.

```{r}
str(colleges)
```
